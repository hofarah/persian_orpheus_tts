{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YtaTcUEaJJK",
        "outputId": "de8c1732-4c8a-468c-8bf1-7bdc9104880d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Downloading torchcodec-0.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install json_repair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-S6wDIMka8k",
        "outputId": "c77601f2-b0b7-419a-e9af-a0f3f5f5d1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting json_repair\n",
            "  Downloading json_repair-0.52.3-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading json_repair-0.52.3-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: json_repair\n",
            "Successfully installed json_repair-0.52.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -U"
      ],
      "metadata": {
        "id": "jpWdiIAdoDfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def persian_to_finglish(text):\n",
        "    mapping = {\n",
        "        'ا': 'a', 'آ': 'aa', 'ب': 'b', 'پ': 'p', 'ت': 't', 'ث': 's', 'ج': 'j',\n",
        "        'چ': 'ch', 'ح': 'h', 'خ': 'kh', 'د': 'd', 'ذ': 'z', 'ر': 'r', 'ز': 'z',\n",
        "        'ژ': 'zh', 'س': 's', 'ش': 'sh', 'ص': 's', 'ض': 'z', 'ط': 't', 'ظ': 'z',\n",
        "        'ع': 'a', 'غ': 'gh', 'ف': 'f', 'ق': 'gh', 'ک': 'k', 'گ': 'g', 'ل': 'l',\n",
        "        'م': 'm', 'ن': 'n', 'و': 'v', 'ه': 'h', 'ی': 'i', 'ء': '', ' ': ' ',\n",
        "        'ً': '', 'ٌ': '', 'ٍ': '', 'َ': 'a', 'ُ': 'o', 'ِ': 'e', 'ّ': '', 'ٔ': ''\n",
        "    }\n",
        "\n",
        "    result = \"\"\n",
        "    for ch in text:\n",
        "        result += mapping.get(ch, ch)\n",
        "    return result\n",
        "\n",
        "def persian_to_finglish_list(text_list):\n",
        "    return [persian_to_finglish(text) for text in text_list]"
      ],
      "metadata": {
        "id": "z1pXeeNjKSFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json_repair\n",
        "\n",
        "def json_decode(input_data):\n",
        "    return json_repair.repair_json(input_data, ensure_ascii=False, return_objects=True)\n",
        "\n",
        "client = OpenAI(api_key='your token here')\n",
        "def persian_sentences_to_finglish(sentences: list):\n",
        "    try:\n",
        "        prompt = \"sentences = [\\n\" + \"\\n\".join([f'    \"{sentence}\"' for sentence in sentences]) + \"\\n] Output:\"\n",
        "\n",
        "        messages =[{\"role\":\"system\",\"content\":\"\"\"\n",
        "        Please convert the following list of Persian sentences into Finglish.\n",
        "        The output should be exactly in the same format as the input list.\n",
        "        Make sure to distinguish between short and long vowels.\n",
        "        Example:\n",
        "\n",
        "        Input:\n",
        "        sentences = [\n",
        "            \" حالت خوبه؟ \",\n",
        "            \"سلام چه خبر؟\",\n",
        "            \"خوبی؟\",\n",
        "            \"چه کار می‌کنی؟\",\n",
        "            \"اسلام و ایران\"\n",
        "        ]\n",
        "\n",
        "        Output:\n",
        "        { \"sentences\" : [\n",
        "            \"haalet khoobe?\",\n",
        "            \"salaam che khabar?\",\n",
        "            \"khoobi?\",\n",
        "            \"che kaar mikoni?\",\n",
        "            \"eslaam va iran\"\n",
        "        ]}\n",
        "\n",
        "        Now, convert the following list of sentences into Finglish:\n",
        "        \"\"\"},{\"role\":\"user\",\"content\":prompt}]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-5-mini\",\n",
        "            messages=messages,\n",
        "            reasoning_effort=\"minimal\",\n",
        "            max_completion_tokens=10000,\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content\n",
        "        res = json_decode(content)\n",
        "        return res[\"sentences\"]\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n"
      ],
      "metadata": {
        "id": "YGD1PM_AZ0sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persian_sentences_to_finglish([\"سرمدی گفت که خیلی حالت خوبه\",\"همچنین ابراهیم و اسماعیل ذبح شدند\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqVQjxXJn0V3",
        "outputId": "6e766714-8f8d-4e43-a123-cf5f0c91c807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sarmadi goft ke kheyli haalet khoobe',\n",
              " 'hamchenin ebrahim va esmaeil zabh shodand']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "ff={}\n",
        "with open(\"partial_results.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    ff[record['text']]=record\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "print(len(ff))\n",
        "def find_json_by_text(input_text: str, file_path: str = \"partial_results.jsonl\"):\n",
        "\n",
        "\n",
        "    try:\n",
        "       for record in ff.values():\n",
        "                if record.get(\"text\") == input_text:\n",
        "                    return record\n",
        "    except FileNotFoundError:\n",
        "        print(f\"⚠️ File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error reading {file_path}: {e}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "6EqDOO-ejYi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4945a474-6c3d-4e5d-bb93-62b6f4cd269b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_json_by_text('آب این مکان از چشمه ای تأمین می شود که از دل کوه می جوشد و آب سرد و گوارایی را به رهگذران خسته تقدیم می کند سرسبزی و هوای بسیار مطلوب این محیط باعث شده است که چوپانان این نقطه را برای چرای دام های خود بر گزینند و گله گوسفند و بز خود را در این منطقه بچرانند')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3febeTybgK9",
        "outputId": "d0d381a4-cee7-4ca9-ebbe-b3425451a896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'آب این مکان از چشمه ای تأمین می شود که از دل کوه می جوشد و آب سرد و گوارایی را به رهگذران خسته تقدیم می کند سرسبزی و هوای بسیار مطلوب این محیط باعث شده است که چوپانان این نقطه را برای چرای دام های خود بر گزینند و گله گوسفند و بز خود را در این منطقه بچرانند',\n",
              " 'result': \"aab-e in makan az cheshmeh-i ta’min mishavad ke az del-e kuah mi jushad va aab-e sard va govaaraa'i ra be rahgozar-aan-e khaste taqdim mikonad sarsabzi va hava-ye besyaar matloob-e in mohit baes shodeh ast ke chupaanaan in noghte ra baraye chera-ye daam-haa-ye khod bargizand vagele-ye goosfand va booz-e khod ra dar in manteqe becheraand\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import threading,json\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "from datasets import load_dataset, Audio, DatasetDict, Features, Value\n",
        "\n",
        "login(\"your huggingface token here\")\n",
        "\n",
        "ds = load_dataset(\"saeedzou/PersianVox_NM\", split=\"train[:55%]\", keep_in_memory=False)\n",
        "if \"audio\" in ds.column_names:\n",
        "    ds = ds.cast_column(\"audio\", Audio(decode=False))\n",
        "keep = [c for c in [\"audio\", \"text\",\"id\"] if c in ds.column_names]\n",
        "ds = ds.select_columns(keep)\n",
        "\n",
        "def proc(ex):\n",
        "\n",
        "    try:\n",
        "      mannual = persian_to_finglish_list(ex[\"text\"])\n",
        "\n",
        "      objects= find_json_by_text(ex['text'][-1])\n",
        "      if objects:\n",
        "        print(\"hit\")\n",
        "        t=[find_json_by_text(text)['result'] for text in ex['text']]\n",
        "      else:\n",
        "        print(\"miss\")\n",
        "        t = persian_sentences_to_finglish(ex[\"text\"])\n",
        "        if not t:\n",
        "          t = persian_to_finglish_list(ex[\"text\"])\n",
        "    except:\n",
        "      t=ex[\"text\"]\n",
        "      mannual=ex[\"text\"]\n",
        "\n",
        "    with lock:\n",
        "            with open(\"partial_results.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "                for a, b in zip(ex['text'], t):\n",
        "                    f.write(json.dumps({\"text\": a, \"result\": b}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "    return {\"audio\":ex[\"audio\"],\"text\":ex[\"text\"],\"mannual\":mannual,\"text_gpt5_mini\": t}\n",
        "\n",
        "output_features = Features({\n",
        "    \"audio\": Audio(decode=False),\n",
        "    \"mannual\": Value(\"string\"),\n",
        "    \"text\": Value(\"string\"),\n",
        "    \"text_gpt5_mini\": Value(\"string\"),\n",
        "})\n",
        "\n",
        "ds_small = ds.map(\n",
        "    proc,\n",
        "    batched=True,\n",
        "    batch_size=20,\n",
        "    num_proc=5,\n",
        "    features=output_features,\n",
        "    desc=\"Processing text\"\n",
        ")\n",
        "out = DatasetDict({\"train\": ds_small})\n",
        "out.push_to_hub(\"David-ger/Persian-tts-finglish\")\n",
        "print(\"✅ Pushed!\")"
      ],
      "metadata": {
        "id": "4_Y9D_Ufmilf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}